{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from pickle import load, dump\n",
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelwiseNormalization(nn.Module):\n",
    "    def pixel_norm(self, x):\n",
    "        eps = 1e-8\n",
    "        return x * torch.rsqrt(torch.mean(x * x, 1, keepdim=True) + eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pixel_norm(x)\n",
    "\n",
    "# Adaptive Instance Normalization\n",
    "class AdaIn(nn.Module):\n",
    "    def __init__(self, n_channel, dim_latent):\n",
    "        super().__init__()\n",
    "        self.norm = nn.InstanceNorm2d(n_channel)\n",
    "        self.transform = nn.Linear(dim_latent, n_channel * 2)\n",
    "        self.transform.bias.data[n_channel:] = 0\n",
    "        self.transform.bias.data[:n_channel] = 1\n",
    "        \n",
    "    def forward(self, image, style):\n",
    "        factor, bias = self.transform(style).unsqueeze(2).unsqueeze(3).chunk(2, 1)\n",
    "        result = self.norm(image)\n",
    "        result = result * factor + bias\n",
    "        return result\n",
    "\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, image):\n",
    "        noise = image.new_empty(image.size(0), 1, image.size(2), image.size(3)).normal_()\n",
    "        result = image + self.weight * noise\n",
    "        return result\n",
    "\n",
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, num_channels, dim_latent, first=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if first:\n",
    "            self.conv1 = nn.Conv2d(input_nc, output_nc, kernel_size=4, stride=1, padding=3)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(input_nc, output_nc, kernel_size=3, stride=1, padding=1)\n",
    "        self.noise1 = Noise()\n",
    "        self.adain1 = AdaIn(output_nc, dim_latent)\n",
    "        self.activate1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose2d(output_nc, output_nc, kernel_size=4, stride=2, padding=1)  # upsample\n",
    "        self.noise2 = Noise()\n",
    "        self.adain2 = AdaIn(output_nc, dim_latent)\n",
    "        self.activate2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.toRGB = nn.Conv2d(output_nc, num_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, image, style, last=False):\n",
    "        image = self.conv1(image)\n",
    "        image = self.noise1(image)\n",
    "        image = self.adain1(image, style)\n",
    "        image = self.activate1(image)\n",
    "        \n",
    "        image = self.conv2(image)\n",
    "        image = self.noise2(image)\n",
    "        image = self.adain2(image, style)\n",
    "        image = self.activate2(image)\n",
    "        \n",
    "        if last:\n",
    "            image = self.toRGB(image)\n",
    "        return image\n",
    "\n",
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, num_channels, last=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fromRGB = nn.Conv2d(num_channels, input_nc, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        if not last:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Conv2d(input_nc, input_nc, kernel_size=3, stride=1, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(input_nc, output_nc, kernel_size=4, stride=2, padding=1),  # downsample\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Conv2d(input_nc + 1, input_nc, kernel_size=3, stride=1, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(input_nc, output_nc, kernel_size=4, stride=2, padding=1),  # downsample\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.last = last\n",
    "        \n",
    "    def minibatch_standard_deviation(self, x):\n",
    "        eps = 1e-8\n",
    "        return torch.cat([x, torch.sqrt(((x - x.mean())**2).mean() + eps).expand(x.shape[0], 1, *x.shape[2:])], dim=1)\n",
    "\n",
    "    def forward(self, x, first=False):\n",
    "        if first:\n",
    "            x = self.fromRGB(x)\n",
    "        if self.last:\n",
    "            x = self.minibatch_standard_deviation(x)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, dim_latent, num_depth):\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = [PixelwiseNormalization()]\n",
    "            \n",
    "        for _ in range(num_depth):\n",
    "            modules += [nn.Linear(dim_latent, dim_latent)]\n",
    "            modules += [nn.LeakyReLU(0.2)]\n",
    "            \n",
    "        self.module = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.module(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_depth, num_channels, num_fmap, num_mapping, input_size=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if input_size is None:\n",
    "            self.input_size = num_fmap(0)\n",
    "        else:\n",
    "            self.input_size = input_size\n",
    "            \n",
    "        self.constant_input = nn.Parameter(torch.ones((1, self.input_size), dtype=torch.float32))\n",
    "        \n",
    "        self.style = MappingNetwork(self.input_size, num_mapping)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [GeneratorBlock(self.input_size, num_fmap(1), num_channels, self.input_size, first=True)]\n",
    "            + [GeneratorBlock(num_fmap(i), num_fmap(i + 1), num_channels, self.input_size) for i in range(1, num_depth)])\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "        self.depth = 0\n",
    "        self.alpha = 1.0\n",
    "\n",
    "    def forward(self, styles, input_is_style=False):\n",
    "        if not input_is_style:\n",
    "            styles = [self.style(z) for z in styles]\n",
    "        for _ in range(len(self.blocks) - len(styles)):\n",
    "            styles += [styles[-1]]\n",
    "        \n",
    "        x = self.constant_input.expand(styles[0].size(0), self.input_size).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        rgb = x = self.blocks[0](x, styles[0], self.depth == 0)\n",
    "        \n",
    "        if self.depth > 0:\n",
    "            for i in range(self.depth - 1):\n",
    "                x = self.blocks[i+1](x, styles[i+1])\n",
    "            rgb = self.blocks[self.depth](x, styles[self.depth], last=True)\n",
    "            if self.alpha < 1.0:\n",
    "                prev_rgb = self.blocks[self.depth - 1].toRGB(x)\n",
    "                prev_rgb = F.interpolate(prev_rgb, mode='bilinear', scale_factor=2, align_corners=True, recompute_scale_factor=True)\n",
    "                rgb = (1 - self.alpha) * prev_rgb + self.alpha * rgb\n",
    "            \n",
    "        rgb = self.activation(rgb)\n",
    "        \n",
    "        return rgb, styles\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_depth, num_channels, num_fmap):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [DiscriminatorBlock(num_fmap(i), num_fmap(i-1), num_channels) for i in range(num_depth, 1, -1)]\n",
    "            + [DiscriminatorBlock(num_fmap(1), num_fmap(0), num_channels, last=True)])\n",
    "\n",
    "        # PatchGAN\n",
    "        self.conv_last = nn.Conv2d(num_fmap(0), 1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.depth = 0\n",
    "        self.alpha = 1.0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.blocks[-(self.depth + 1)](x, first=True)\n",
    "        \n",
    "        if self.depth > 0 and self.alpha < 1.0:\n",
    "            x = F.interpolate(x, mode='bilinear', scale_factor=0.5, align_corners=True, recompute_scale_factor=True)\n",
    "            prev = self.blocks[-self.depth].fromRGB(x)\n",
    "            out = self.alpha * out + (1 - self.alpha) * prev\n",
    "                \n",
    "        for i in range(self.depth, 0, -1):\n",
    "            out = self.blocks[-i](out)\n",
    "            \n",
    "        out = self.conv_last(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(image):\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    PIL = transforms.ToPILImage()\n",
    "    ToTensor = transforms.ToTensor()\n",
    "\n",
    "    img = PIL(image)\n",
    "    fig = plt.figure(dpi=200)\n",
    "    ax = fig.add_subplot(1, 1, 1) # (row, col, num)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = 'cpu'#torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "num_mapping = 8\n",
    "image_size = 128\n",
    "max_depth = int(np.log2(image_size)) - 2\n",
    "def num_fmap(stage):\n",
    "    base_size = image_size * 2\n",
    "    fmap_base = base_size * 4\n",
    "    fmap_max = base_size // 2\n",
    "    fmap_decay = 1.0\n",
    "    return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
    "feed_dim = num_fmap(0)\n",
    "net = Generator(max_depth, 3, num_fmap, num_mapping).to(device)\n",
    "net.depth = 4\n",
    "net.load_state_dict(torch.load('weight_G.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_feeds = [torch.randn(1, feed_dim).to(device)]\n",
    "image, style = net(style_feeds)\n",
    "showImage(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds1 = style_feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_feeds = [torch.randn(1, feed_dim).to(device)]\n",
    "image, style = net(style_feeds)\n",
    "showImage(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds2 = style_feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig = plt.figure(dpi=200)\n",
    "ax = fig.add_subplot(1, 1, 1) # (row, col, num)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "images = []\n",
    "for i in range(300):\n",
    "    l = i / 300.0\n",
    "    x = l * feeds1[0] + (1 - l) * feeds2[0]\n",
    "    image, _ = net([x])\n",
    "    PIL = transforms.ToPILImage()\n",
    "    img = PIL(image[0])\n",
    "    images += [[plt.imshow(img, animated=True)]]\n",
    "ani = animation.ArtistAnimation(fig, images, interval=10, repeat_delay=1000)\n",
    "ani.save('anim.gif', writer=\"imagemagick\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
