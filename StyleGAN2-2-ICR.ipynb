{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ccpy4OkFMEM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from pickle import load, dump\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightScaledConv2d(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, kernel_size, stride=1, padding=0, groups=1):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(output_nc, input_nc//groups, kernel_size, kernel_size))\n",
    "        self.scale = np.sqrt(2 / (input_nc * kernel_size ** 2))\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "\n",
    "        self.upsample = False\n",
    "        \n",
    "    def deconv(self):\n",
    "        self.upsample = True\n",
    "        return self\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weight = self.weight * self.scale\n",
    "        if not self.upsample:\n",
    "            out = F.conv2d(x, weight=weight, stride=self.stride, padding=self.padding, groups=self.groups)\n",
    "        else:\n",
    "            weight = weight.transpose(0, 1)\n",
    "            out = F.conv_transpose2d(x, weight=weight, stride=self.stride, padding=self.padding, groups=self.groups)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightScaledLinear(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, bias=False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(output_nc, input_nc))\n",
    "        self.scale = np.sqrt(2 / input_nc)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(output_nc))\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weight = self.weight * self.scale\n",
    "        out = F.linear(x, weight=weight, bias=self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        channel = x.size(1)\n",
    "        assert channel % 2 == 0, 'must divide by 2.'\n",
    "        return x[:, :channel//2] * torch.sigmoid(x[:, channel//2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FReLU(nn.Module):\n",
    "    def __init__(self, n_channel, kernel=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.funnel_condition = WeightScaledConv2d(n_channel, n_channel, kernel_size=kernel,stride=stride, padding=padding, groups=n_channel)\n",
    "        self.bn = nn.BatchNorm2d(n_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tx = self.bn(self.funnel_condition(x))\n",
    "        out = torch.max(x, tx)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    @staticmethod\n",
    "    def mish(x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return Mish.mish(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pointwise Convolution\n",
    "        self.query_conv = nn.Conv2d(input_nc, input_nc // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(input_nc, input_nc // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(input_nc, input_nc, kernel_size=1)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x, return_map=False):\n",
    "        proj_query = self.query_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3]).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3])\n",
    "        s = torch.bmm(proj_query, proj_key)\n",
    "        attention_map_T = self.softmax(s)\n",
    "        \n",
    "        proj_value = self.value_conv(x).view(x.shape[0], -1, x.shape[2] * x.shape[3])\n",
    "        o = torch.bmm(proj_value, attention_map_T)\n",
    "        \n",
    "        o = o.view(x.shape[0], x.shape[1], x.shape[2], x.shape[3])\n",
    "        out = x + self.gamma * o\n",
    "        \n",
    "        if return_map:\n",
    "            return out, attention_map_T.permute(0, 2, 1)\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelwiseNormalization(nn.Module):\n",
    "    def pixel_norm(self, x):\n",
    "        eps = 1e-8\n",
    "        return x * torch.rsqrt(torch.mean(x * x, 1, keepdim=True) + eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pixel_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModulatedConv2d(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, dim_latent, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.epsilon = 1e-8\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.norm = nn.InstanceNorm2d(input_nc)\n",
    "        self.weight = nn.Parameter(torch.randn([1, output_nc, input_nc, kernel_size, kernel_size]))\n",
    "        self.scale = np.sqrt(2 / (input_nc * kernel_size ** 2))\n",
    "        self.modulate = WeightScaledLinear(dim_latent, input_nc)\n",
    "        \n",
    "        self.upsample = False\n",
    "        \n",
    "    def deconv(self):\n",
    "        self.upsample = True\n",
    "        return self\n",
    "        \n",
    "    def forward(self, image, style):\n",
    "        batch, input_nc, height, width = image.shape\n",
    "        _, output_nc, _, kernel_size, _ = self.weight.shape\n",
    "        \n",
    "        style = self.modulate(style)\n",
    "        weight = self.weight * self.scale * style.view(batch, 1, input_nc, 1, 1)\n",
    "        \n",
    "        # demodulation\n",
    "        demodulate = (self.weight.square().sum([2, 3, 4]) + self.epsilon).rsqrt().view(1, output_nc, 1, 1, 1)\n",
    "        weight = weight * demodulate\n",
    "        \n",
    "        image = image.reshape(1, batch * input_nc, height, width)\n",
    "        \n",
    "        if not self.upsample:\n",
    "            weight = weight.view(batch * output_nc, input_nc, kernel_size, kernel_size)\n",
    "            out = F.conv2d(image, weight=weight, bias=None, groups=batch, stride=self.stride, padding=self.padding)\n",
    "        else:\n",
    "            weight = weight.transpose(1, 2)\n",
    "            weight = weight.reshape(batch * input_nc, output_nc, kernel_size, kernel_size)\n",
    "            out = F.conv_transpose2d(image, weight=weight, bias=None, groups=batch, stride=self.stride, padding=self.padding)\n",
    "        \n",
    "        out = out.view(batch, output_nc, out.size(2), out.size(3))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, image):\n",
    "        noise = image.new_empty(image.size(0), 1, image.size(2), image.size(3)).normal_()\n",
    "        result = image + self.weight * noise\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_channel, dim_latent):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = ModulatedConv2d(input_nc, output_nc, dim_latent, kernel_size=4, stride=2, padding=1).deconv()  # upsample\n",
    "        self.noise1 = Noise()\n",
    "        self.normalize1 = PixelwiseNormalization()\n",
    "        self.activate1 = FReLU(output_nc)\n",
    "        \n",
    "        #self.attention = SelfAttention(output_nc)\n",
    "        \n",
    "        self.conv2 = ModulatedConv2d(output_nc, output_nc, dim_latent, kernel_size=3, stride=1, padding=1)\n",
    "        self.noise2 = Noise()\n",
    "        self.normalize2 = PixelwiseNormalization()\n",
    "        self.activate2 = FReLU(output_nc)\n",
    "        \n",
    "        self.toRGB = WeightScaledConv2d(output_nc, n_channel, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, image, style):\n",
    "        image = self.conv1(image, style)\n",
    "        image = self.noise1(image)\n",
    "        image = self.normalize1(image)\n",
    "        image = self.activate1(image)\n",
    "        \n",
    "        #image = self.attention(image)\n",
    "        \n",
    "        image = self.conv2(image, style)\n",
    "        image = self.noise2(image)\n",
    "        image = self.normalize2(image)\n",
    "        image = self.activate2(image)\n",
    "        \n",
    "        rgb = self.toRGB(image)\n",
    "        \n",
    "        return image, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            WeightScaledConv2d(input_nc, output_nc, kernel_size=4, stride=2, padding=1),  # downsample\n",
    "            PixelwiseNormalization(),\n",
    "            Mish(),\n",
    "            #SelfAttention(output_nc),\n",
    "            WeightScaledConv2d(output_nc, output_nc, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.skip_conv = WeightScaledConv2d(input_nc, output_nc, kernel_size=3, stride=1, padding=1)\n",
    "            \n",
    "        self.activation = nn.Sequential(\n",
    "            PixelwiseNormalization(),\n",
    "            Mish()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        \n",
    "        bilinear = F.interpolate(x, mode='bilinear', scale_factor=0.5, align_corners=True, recompute_scale_factor=True)\n",
    "        skip = self.skip_conv(bilinear)\n",
    "        \n",
    "        out = out + skip\n",
    "        out = self.activation(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, dim_latent, num_depth):\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = []\n",
    "        \n",
    "        for _ in range(num_depth):\n",
    "            modules += [WeightScaledLinear(dim_latent, dim_latent)]\n",
    "            modules += [PixelwiseNormalization()]\n",
    "            modules += [Mish()]\n",
    "        \n",
    "        self.module = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d53nPWZFheB4"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_depth, num_fmap, num_mapping, small_size, n_channel=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = num_fmap(0)\n",
    "        self.small_size = small_size\n",
    "        self.register_buffer('const', torch.ones((1, self.input_size, 2, 2), dtype=torch.float32))\n",
    "        \n",
    "        self.style = MappingNetwork(self.input_size, num_mapping)\n",
    "        self.blocks = nn.ModuleList([GeneratorBlock(num_fmap(i), num_fmap(i+1), n_channel, self.input_size) for i in range(num_depth)])\n",
    "        \n",
    "    def forward(self, styles, input_is_style=False):\n",
    "        if not input_is_style:\n",
    "            styles = [self.style(z) for z in styles]\n",
    "        for _ in range(len(self.blocks) - len(styles)):\n",
    "            styles += [styles[-1]]\n",
    "        styles = [style.unsqueeze(1) for style in styles]\n",
    "        styles = torch.cat(styles, dim=1).to(styles[0].device)\n",
    "        \n",
    "        x = self.const.expand(styles.size(0), self.input_size, 2, 2)\n",
    "        \n",
    "        prev_rgb = None\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x, rgb = block(x, styles[:,i,:])\n",
    "            if prev_rgb is not None:\n",
    "                upsampled = F.interpolate(prev_rgb, mode='bilinear', scale_factor=2, align_corners=True, recompute_scale_factor=True)\n",
    "                rgb = rgb + upsampled  # Skip Connection\n",
    "            prev_rgb = rgb\n",
    "            if rgb.size(-1) == self.small_size:\n",
    "                rgb_small = rgb\n",
    "        \n",
    "        rgb_small = torch.sigmoid(rgb_small)\n",
    "        rgb = torch.sigmoid(rgb)\n",
    "        \n",
    "        return rgb, rgb_small, styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    class BasicBlock(nn.Module):\n",
    "        def __init__(self, dim_in, dim_out):\n",
    "            super().__init__()\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                WeightScaledConv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "                PixelwiseNormalization(),\n",
    "                FReLU(dim_out)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.block(x)\n",
    "    \n",
    "    def __init__(self, input_nc, n_channel=3):\n",
    "        super().__init__()\n",
    "        self.block1 = SimpleDecoder.BasicBlock(input_nc, input_nc)\n",
    "        self.block2 = SimpleDecoder.BasicBlock(input_nc, input_nc)\n",
    "        self.block3 = SimpleDecoder.BasicBlock(input_nc, input_nc)\n",
    "        self.block4 = SimpleDecoder.BasicBlock(input_nc, input_nc)\n",
    "        \n",
    "        self.toRGB = nn.Sequential(\n",
    "            WeightScaledConv2d(input_nc, n_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.toRGB(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0TccaIsSJxd"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_depth, num_fmap, small_size, n_channel=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.small_size = small_size\n",
    "        self.fromRGB = WeightScaledConv2d(n_channel, num_fmap(num_depth), kernel_size=1, stride=1, padding=0)\n",
    "        self.fromRGB_small = nn.Conv2d(n_channel, num_fmap(np.log2(small_size)), kernel_size=1, stride=1, padding=0)\n",
    "        self.blocks = nn.ModuleList([DiscriminatorBlock(num_fmap(i+1), num_fmap(i)) for i in range(num_depth)][::-1])\n",
    "        \n",
    "        self.decoder1 = SimpleDecoder(num_fmap(np.log2(small_size) - 3))\n",
    "        self.decoder2 = SimpleDecoder(num_fmap(np.log2(small_size) - 4))\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "        # PatchGAN\n",
    "        self.conv_last = WeightScaledConv2d(num_fmap(0)+1, 1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def minibatch_standard_deviation(self, x):\n",
    "        eps = 1e-8\n",
    "        return torch.cat([x, torch.sqrt(((x - x.mean())**2).mean() + eps).expand(x.shape[0], 1, *x.shape[2:])], dim=1)\n",
    "    \n",
    "    def forward(self, x, is_real=False):\n",
    "        if is_real:\n",
    "            real_small = F.interpolate(x, size=(self.small_size, self.small_size))\n",
    "            real_small_2 = F.interpolate(x, size=(self.small_size * 2, self.small_size * 2))\n",
    "        else:\n",
    "            x_small = x[1]\n",
    "            x_small = self.fromRGB_small(x_small)\n",
    "            x = x[0]\n",
    "            \n",
    "        x = self.fromRGB(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "            if is_real:\n",
    "                if x.size(-1) * (2 ** 3) == self.small_size:\n",
    "                    x_crop, pos = Util.randomCrop(x, self.small_size // 16)\n",
    "                    fake_crop_small = self.decoder1(x_crop)\n",
    "                if x.size(-1) * (2 ** 4) == self.small_size:\n",
    "                    fake_small = self.decoder2(x)\n",
    "            elif x.size(-1) <= self.small_size:\n",
    "                x_small = block(x_small)\n",
    "        \n",
    "        x = self.minibatch_standard_deviation(x)\n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        if is_real:\n",
    "            real_crop_small = Util.crop(real_small_2, self.small_size, pos * 16)\n",
    "            loss_recon = self.mse_loss(real_small, fake_small) + self.mse_loss(real_crop_small, fake_crop_small)\n",
    "            return out, loss_recon\n",
    "        else:\n",
    "            x_small = self.minibatch_standard_deviation(x_small)\n",
    "            out_small = self.conv_last(x_small)\n",
    "            return out, out_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "    @staticmethod\n",
    "    def randomCrop(image, size):\n",
    "        h = random.randrange(2) * size\n",
    "        w = random.randrange(2) * size\n",
    "        image = image[:, :, h:h+size, w:w+size]\n",
    "        return image, (h, w)\n",
    "    \n",
    "    @staticmethod\n",
    "    def crop(image, size, pos):\n",
    "        image = image[:, :, pos[0]:pos[0]+size, pos[1]:pos[1]+size]\n",
    "        return image\n",
    "    \n",
    "    @staticmethod\n",
    "    def loadImages(batch_size, folder_path, size):\n",
    "        imgs = ImageFolder(folder_path, transform=transforms.Compose([\n",
    "            transforms.Resize(int(size)),\n",
    "            transforms.RandomCrop(size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "        return DataLoader(imgs, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def augment(images):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ColorJitter(brightness=0, contrast=0.5, saturation=0.5),\n",
    "            transforms.RandomRotation(degrees=30),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomErasing()\n",
    "        ])\n",
    "        device = images.device\n",
    "        return torch.cat([transform(img).unsqueeze(0) for img in images.cpu()], 0).to(device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def showImage(image):\n",
    "        %matplotlib inline\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        PIL = transforms.ToPILImage()\n",
    "        ToTensor = transforms.ToTensor()\n",
    "        \n",
    "        img = PIL(image)\n",
    "        fig = plt.figure(dpi=200)\n",
    "        ax = fig.add_subplot(1, 1, 1) # (row, col, num)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(img)\n",
    "        #plt.gray()\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def showImages(dataloader):\n",
    "        for images in dataloader:\n",
    "            for image in images[0]:\n",
    "                Util.showImage(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, args):\n",
    "        use_cuda = torch.cuda.is_available() if not args.cpu else False\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(f'Use Device: {self.device}')\n",
    "        \n",
    "        def num_fmap(stage):\n",
    "            base_size = self.args.image_size\n",
    "            fmap_base = base_size * 4\n",
    "            fmap_max = base_size // 2\n",
    "            fmap_decay = 1.0\n",
    "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
    "        \n",
    "        self.args = args\n",
    "        self.feed_dim = num_fmap(0)\n",
    "        self.max_depth = int(np.log2(self.args.image_size)) - 1\n",
    "        small_size = args.image_size // 4\n",
    "        \n",
    "        self.netG = Generator(self.max_depth, num_fmap, self.args.num_mapping, small_size).to(self.device)\n",
    "        self.netD = Discriminator(self.max_depth, num_fmap, small_size).to(self.device)\n",
    "        self.state_loaded = False\n",
    "\n",
    "        self.netG.apply(self.weights_init)\n",
    "        self.netD.apply(self.weights_init)\n",
    "\n",
    "        self.optimizer_G = optim.Adam(self.netG.parameters(), lr=self.args.lr, betas=(0, 0.9))\n",
    "        self.optimizer_D = optim.Adam(self.netD.parameters(), lr=self.args.lr * self.args.mul_lr_dis, betas=(0, 0.9))\n",
    "        #self.scheduler_G = CosineAnnealingLR(self.optimizer_G, T_max=4, eta_min=self.lr/4)\n",
    "        #self.scheduler_D = CosineAnnealingLR(self.optimizer_D, T_max=4, eta_min=(self.lr * self.args.mul_lr_dis)/4)\n",
    "        \n",
    "        self.load_dataset()\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.ConvTranspose2d:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias != None:\n",
    "                module.bias.data.fill_(0)\n",
    "            \n",
    "    def load_dataset(self):\n",
    "        self.dataloader = Util.loadImages(self.args.batch_size, self.args.image_dir, self.args.image_size)\n",
    "        self.max_iters = len(iter(self.dataloader))\n",
    "            \n",
    "    def save_state(self, epoch):\n",
    "        self.netG.cpu(), self.netD.cpu()\n",
    "        torch.save(self.netG.state_dict(), os.path.join(self.args.weight_dir, f'weight_G.{epoch}.pth'))\n",
    "        torch.save(self.netD.state_dict(), os.path.join(self.args.weight_dir, f'weight_D.{epoch}.pth'))\n",
    "        self.netG.to(self.device), self.netD.to(self.device)\n",
    "        \n",
    "    def load_state(self):\n",
    "        if (os.path.exists('weight_G.pth') and os.path.exists('weight_D.pth')):\n",
    "            self.netG.load_state_dict(torch.load('weight_G.pth', map_location=self.device))\n",
    "            self.netD.load_state_dict(torch.load('weight_D.pth', map_location=self.device))\n",
    "            self.state_loaded = True\n",
    "            print('Loaded network state.')\n",
    "    \n",
    "    def save_resume(self):\n",
    "        with open(os.path.join('.', f'resume.pkl'), 'wb') as f:\n",
    "            dump(self, f)\n",
    "    \n",
    "    def load_resume(self):\n",
    "        if os.path.exists('resume.pkl'):\n",
    "            with open(os.path.join('.', 'resume.pkl'), 'rb') as f:\n",
    "                print('Load resume.')\n",
    "                return load(f)\n",
    "        else:\n",
    "            return self\n",
    "        \n",
    "    def trainGAN(self, epoch, iters, max_iters, real_img, a=0, b=1, c=1):\n",
    "        ### Train with LSGAN.\n",
    "        ### for example, (a, b, c) = 0, 1, 1 or (a, b, c) = -1, 1, 0\n",
    "        \n",
    "        mse_loss = nn.MSELoss()\n",
    "        style_feeds = [torch.randn(real_img.size(0), self.feed_dim).to(self.device)]\n",
    "        noise = torch.Tensor(np.random.normal(0, self.args.lambda_zcr_noise, (real_img.size(0), self.feed_dim))).to(self.device)\n",
    "        z = [style + noise for style in style_feeds]\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                             Train the discriminator                              #\n",
    "        # ================================================================================ #\n",
    "        \n",
    "        # Compute loss with real images.\n",
    "        real_img_aug = Util.augment(real_img)\n",
    "        real_src_score, d_loss_recon = self.netD(real_img_aug, is_real=True)\n",
    "        real_src_loss = torch.sum((real_src_score - b) ** 2)\n",
    "        \n",
    "        # Compute loss with fake images.\n",
    "        fake_img, fake_img_small, _ = self.netG(style_feeds)\n",
    "        fake_src_score, fake_small_score = self.netD((fake_img, fake_img_small))\n",
    "        fake_src_loss = torch.sum((fake_src_score - a) ** 2)\n",
    "        fake_small_loss = torch.sum((fake_small_score - a) ** 2)\n",
    "        \n",
    "        bcr_real = mse_loss(self.netD(real_img, is_real=True)[0], real_src_score)\n",
    "        fake_img_aug = Util.augment(fake_img)\n",
    "        fake_img_small_aug = Util.augment(fake_img_small)\n",
    "        fake_src_score_aug, fake_small_score_aug = self.netD((fake_img_aug, fake_img_small_aug))\n",
    "        bcr_fake = mse_loss(fake_src_score_aug, fake_src_score) + mse_loss(fake_small_score_aug, fake_small_score)\n",
    "        \n",
    "        z_img, z_small_img, _ = self.netG(z)\n",
    "        z_score, z_small_score = self.netD((z_img, z_small_img))\n",
    "        zcr_loss = mse_loss(fake_src_score, z_score) + mse_loss(fake_small_score, z_small_score)\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        d_loss = (0.5 * (real_src_loss + fake_src_loss + fake_small_loss) / self.args.batch_size + d_loss_recon\n",
    "                  + self.args.lambda_bcr_real * bcr_real + self.args.lambda_bcr_fake * bcr_fake + self.args.lambda_zcr_dis * zcr_loss)\n",
    "        self.optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optimizer_D.step()\n",
    "              \n",
    "        # Logging.\n",
    "        loss = {}\n",
    "        loss['D/loss'] = d_loss.item()\n",
    "        loss['D/bcr_loss'] = (bcr_real + bcr_fake).item()\n",
    "        loss['D/zcr_loss'] = zcr_loss.item()\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                               Train the generator                                #\n",
    "        # ================================================================================ #\n",
    "        # Compute loss with reconstruction loss\n",
    "        fake_img, fake_img_small, styles = self.netG(style_feeds)\n",
    "        fake_src_score, fake_small_score = self.netD((fake_img, fake_img_small))\n",
    "        fake_src_loss = torch.sum((fake_src_score - c) ** 2)\n",
    "        fake_small_loss = torch.sum((fake_small_score - c) ** 2)\n",
    "        \n",
    "        z_img, z_small_img, _ = self.netG(z)\n",
    "        zcr_loss = - mse_loss(fake_img, z_img) - mse_loss(fake_img_small, z_small_img)\n",
    "\n",
    "        # Compute loss for path regularization\n",
    "        noise = torch.randn_like(fake_img) / np.sqrt(fake_img.shape[2] * fake_img.shape[3])\n",
    "        grad, = torch.autograd.grad(outputs=(fake_img * noise).sum(), inputs=styles, create_graph=True)\n",
    "        path_length = grad.norm(2, dim=2).mean(1)\n",
    "        path_mean = self.mean_path_length + 0.01 * (path_length.mean() - self.mean_path_length)\n",
    "        path_penalty = (path_length - path_mean).square().mean()\n",
    "        self.mean_path_length = path_mean.detach()\n",
    "\n",
    "        # Backward and optimize.\n",
    "        g_loss = 0.5 * (fake_src_loss + fake_small_loss) / self.args.batch_size + self.args.lambda_path * path_penalty + self.args.lambda_zcr_gen * zcr_loss\n",
    "        self.optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        # Logging.\n",
    "        loss['G/loss'] = g_loss.item()\n",
    "        loss['G/path_penalty'] = path_penalty.item()\n",
    "        loss['G/zcr_loss'] = zcr_loss.item()\n",
    "        \n",
    "        # Save\n",
    "        if iters == max_iters:\n",
    "            self.save_state(epoch)\n",
    "            img_name = str(epoch) + '_' + str(iters) + '.png'\n",
    "            img_path = os.path.join(self.args.result_dir, img_name)\n",
    "            save_image(fake_img, img_path)\n",
    "            #Util.showImage(fake_img)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, resume=True):\n",
    "        self.netG.train()\n",
    "        self.netD.train()\n",
    "        \n",
    "        while self.args.num_train > self.epoch:\n",
    "            self.epoch += 1\n",
    "            epoch_loss_G = 0.0\n",
    "            epoch_loss_D = 0.0\n",
    "            \n",
    "            self.mean_path_length = 0\n",
    "            for iters, (data, _) in enumerate(tqdm(self.dataloader)):\n",
    "                iters += 1\n",
    "                \n",
    "                data = data.to(self.device)\n",
    "                \n",
    "                loss = self.trainGAN(self.epoch, iters, self.max_iters, data)\n",
    "                \n",
    "                epoch_loss_D += loss['D/loss']\n",
    "                epoch_loss_G += loss['G/loss']\n",
    "                #experiment.log_metrics(loss)\n",
    "                \n",
    "            #self.scheduler_G.step()\n",
    "            #self.scheduler_D.step()\n",
    "            \n",
    "            epoch_loss = epoch_loss_G + epoch_loss_D\n",
    "            \n",
    "            print(f'Epoch[{self.epoch}]'\n",
    "                  #+ f' LR[G({self.scheduler_G.get_last_lr()[0]:.5f}) D({self.scheduler_D.get_last_lr()[0]:.5f})]'\n",
    "                  + f' Loss[G({epoch_loss_G}) + D({epoch_loss_D}) = {epoch_loss}]')\n",
    "                    \n",
    "            if resume:\n",
    "                self.save_resume()\n",
    "    \n",
    "    def generate(self, num=100):\n",
    "        self.netG.eval()\n",
    "        \n",
    "        for _ in range(num):\n",
    "            random_data = [torch.randn(1, self.netG.input_size).to(self.device)]\n",
    "            fake_img = self.netG(random_data)[0][0,:]\n",
    "            save_image(fake_img, os.path.join(self.args.result_dir, f'generated_{time.time()}.png'))\n",
    "            #Util.showImage(fake_img)\n",
    "        print('New picture was generated.')\n",
    "        \n",
    "    def showImages(self):\n",
    "        Util.showImages(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    hyper_params = {}\n",
    "    hyper_params['Image Dir'] = args.image_dir\n",
    "    hyper_params['Result Dir'] = args.result_dir\n",
    "    hyper_params['Weight Dir'] = args.weight_dir\n",
    "    hyper_params['Image Size'] = args.image_size\n",
    "    hyper_params['Learning Rate'] = args.lr\n",
    "    hyper_params[\"Mul Discriminator's LR\"] = args.mul_lr_dis\n",
    "    hyper_params['Batch Size'] = args.batch_size\n",
    "    hyper_params['Num Train'] = args.num_train\n",
    "    hyper_params['Num Mapping Net'] = args.num_mapping\n",
    "    hyper_params['Path Regularize Coef'] = args.lambda_path\n",
    "    hyper_params['bCR lambda_real'] = args.lambda_bcr_real\n",
    "    hyper_params['bCR lambda_fake'] = args.lambda_bcr_fake\n",
    "    hyper_params['zCR lambda_gen'] = args.lambda_zcr_gen\n",
    "    hyper_params['zCR lambda_dis'] = args.lambda_zcr_dis\n",
    "    hyper_params['zCR lambda_noise'] = args.lambda_zcr_noise\n",
    "    \n",
    "    solver = Solver(args)\n",
    "    solver.load_state()\n",
    "    \n",
    "    if not args.noresume:\n",
    "        solver = solver.load_resume()\n",
    "    \n",
    "    if args.generate > 0:\n",
    "        solver.generate(args.generate)\n",
    "        return\n",
    "        \n",
    "    for key in hyper_params.keys():\n",
    "        print(f'{key}: {hyper_params[key]}')\n",
    "    #experiment.log_parameters(hyper_params)\n",
    "    \n",
    "    #solver.showImages()\n",
    "    solver.train(not args.noresume)\n",
    "    #experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image_dir', type=str, default='')\n",
    "    parser.add_argument('--result_dir', type=str, default='results')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--image_size', type=int, default=256)\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--mul_lr_dis', type=float, default=4)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "    parser.add_argument('--num_train', type=int, default=100)\n",
    "    parser.add_argument('--num_mapping', type=int, default=8)\n",
    "    parser.add_argument('--lambda_path', type=float, default=2)\n",
    "    parser.add_argument('--lambda_bcr_real', type=float, default=10)\n",
    "    parser.add_argument('--lambda_bcr_fake', type=float, default=10)\n",
    "    parser.add_argument('--lambda_zcr_noise', type=float, default=0.07)\n",
    "    parser.add_argument('--lambda_zcr_dis', type=float, default=20)\n",
    "    parser.add_argument('--lambda_zcr_gen', type=float, default=0.5)\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    parser.add_argument('--generate', type=int, default=0)\n",
    "    parser.add_argument('--noresume', action='store_true')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if not os.path.exists(args.result_dir):\n",
    "        os.mkdir(args.result_dir)\n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
